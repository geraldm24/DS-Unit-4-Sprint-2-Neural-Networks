{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "outputs": [],
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "https://www.wandb.com/articles/fundamentals-of-neural-networks\n",
    "### Input Layer: This should be your relevant features you want to train on to make predictions. Your features will each have a neuron.\n",
    "\n",
    "### Hidden Layer: Are the layers between the input and output layers not seen by the external system but gives further indication to your output layer what you are trying to predict. For instance if you are predicting a dog and there are pictures of cats that look very similar. Some hidden layers to may add are tail that are less bendable. Ears that fold over the canal larger sizes. Mouth structure.\n",
    "\n",
    "### Output Layer: The amount of predictions you want to make. For classification it will be binary for an example covid or no covid. Or for regression this can be one target/output layer. For an example, with housing you want to predict the price based on sqft and or bathrooms. \n",
    "\n",
    "### Neuron: Is the building block of a neural network. You multiply the weight by the input and the bias is then added to the weighted input. The sum is passed through an activation function to display an output in a predictable form.\n",
    "\n",
    "### Weight: Can be seen as your coefficient, different features weight more towards your output layer. \n",
    "\n",
    "### Activation Function: An example activation function we use so far was sigmoid which compresses the output between 0,1 to scale output to make them more comparable. \n",
    "### Node Map: Your visual display of the neural netwrok giving representation of your input, hidden, and outputs layers to make a prediction. \n",
    "\n",
    "### Perceptron: Is a type of linear regression. It processes input layers and adjust input weights as instructed by trainer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### A neural network must begin with features that can be generalized to make prediction on called inputs. The input layers are seen but they can also produce inaccurate outputs. We can use neurons to gauge better predictions from a model. Weights can be understood as the coeffiecients that are multiplied to the input layers and then added to a bias to avoid overfitting. These are neurons that can be tweaked. Depending on your model you may use different metrics to determine where bias needs to be added to determine whether your model is making useful predictions. The layers then go through an activation function which is known to scale you layers and put them in a predictable form for your output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "Name: y, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# correct output\n",
    "y = df['y']\n",
    "# activation fucntion\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 +np.exp(-x))\n",
    "\n",
    "def sigmoid_derivate(x):\n",
    "  sx = sigmoid(x)\n",
    "  return sx * (1-sx)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[0:5, [0, 2]].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09927179],\n",
       "       [ 0.12193042]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = 2 * np.random.random((2,1)) -1\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12193042],\n",
       "       [ 0.02265864],\n",
       "       [ 0.12193042],\n",
       "       [-0.09927179]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum of inputs\n",
    "weighted_sum = np.dot(X, weight)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5304449 ],\n",
       "       [0.50566442],\n",
       "       [0.5304449 ],\n",
       "       [0.47520242]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compress weighted inputs for to normalize outputs\n",
    "activated_output = sigmoid(weighted_sum)\n",
    "activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4695551 ],\n",
       "       [ 0.49433558],\n",
       "       [ 0.4695551 ],\n",
       "       [-0.47520242]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# error is obtained of guessed value by actual values\n",
    "y = [[1],[1],[1],[0]]\n",
    "error = y - activated_output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11695355],\n",
       "       [ 0.12356803],\n",
       "       [ 0.11695355],\n",
       "       [-0.11850839]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjustments = error * sigmoid_derivate(weighted_sum)\n",
    "adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09421214],\n",
       "       [ 0.47940555]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight += np.dot(X.T, adjustments)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[-4.36778946]\n",
      " [ 8.9422103 ]]\n",
      "Output after training\n",
      "[[0.99986925]\n",
      " [0.98979247]\n",
      " [0.99986925]\n",
      " [0.01252113]]\n"
     ]
    }
   ],
   "source": [
    "# Steps we've already done: \n",
    "# 1. Randomly Initialized Weights already. Those are in memory as `weights`\n",
    "# 2. We've already got input data & correct_outputs\n",
    "\n",
    "\n",
    "# Update our weights 10,000 times - (fingers crossed that this process reduces error)\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(X, weight)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Cac error\n",
    "    error = y - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivate(weighted_sum)\n",
    "    \n",
    "    # Update the Weights\n",
    "    weight += np.dot(X.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weight)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "763    0\n",
       "764    0\n",
       "765    0\n",
       "766    1\n",
       "767    0\n",
       "Name: Outcome, Length: 768, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "feats\n",
    "\n",
    "X = diabetes[feats]\n",
    "X\n",
    "target = 'Outcome'\n",
    "y2 = diabetes[target]\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "[[0.35294118 0.74371859 0.59016393 ... 0.50074516 0.23441503 0.48333333]\n",
      " [0.05882353 0.42713568 0.54098361 ... 0.39642325 0.11656704 0.16666667]\n",
      " [0.47058824 0.91959799 0.52459016 ... 0.34724292 0.25362938 0.18333333]\n",
      " ...\n",
      " [0.29411765 0.6080402  0.59016393 ... 0.390462   0.07130658 0.15      ]\n",
      " [0.05882353 0.63316583 0.49180328 ... 0.4485842  0.11571307 0.43333333]\n",
      " [0.05882353 0.46733668 0.57377049 ... 0.45305514 0.10119556 0.03333333]]\n",
      "768\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(X))\n",
    "X_scaled = scaler.transform(X)\n",
    "print(X_scaled)\n",
    "print(len(X_scaled))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, rate=0.01, niter = 10):\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 +np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivate(self, x):\n",
    "        sx = sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights # add in some ones for bias\n",
    "        mweights = 2 * np.random.random((8)) -1\n",
    "        # Number of misclassifications\n",
    "        self.error = []  # Number of misclassifications\n",
    "        for i in range(self.niter):\n",
    "            errors = 0\n",
    "            # Weighted sum of inputs / weights\n",
    "            weighted_sum = np.dot(X_scaled, mweights)\n",
    "            # Activate!\n",
    "            activated_output = sigmoid(weighted_sum)\n",
    "            # Cac error\n",
    "            errors = y - activated_output\n",
    "            adjustments = errors * sigmoid_derivate(weighted_sum)\n",
    "            self.error.append(errors)\n",
    "            # Update the Weights\n",
    "            mweights += np.dot(X_scaled.T, adjustments)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        \"\"\" Default Step Function\"\"\"\n",
    "        return activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44398757, -0.47147272,  0.51312959, -0.41926996,  0.6719279 ,\n",
       "       -0.47928729,  0.53461509, -0.55852536,  0.53252467,  0.39429598,\n",
       "       -0.44680096,  0.50387645, -0.54057215,  0.59908433,  0.48446811,\n",
       "        0.46739951,  0.61423823,  0.50833775, -0.48498011,  0.56698482,\n",
       "       -0.38282025, -0.5523409 ,  0.50963243,  0.47854463,  0.4271788 ,\n",
       "        0.44602376,  0.47669612, -0.39431108, -0.630494  , -0.48018046,\n",
       "       -0.58252873,  0.61343757, -0.44025154, -0.47561227, -0.57560808,\n",
       "       -0.43651749, -0.52091036,  0.43605224,  0.55020656,  0.51568641,\n",
       "       -0.44268188, -0.46767138, -0.56786599,  0.5072434 , -0.53026867,\n",
       "        0.66341461, -0.42838276, -0.4254137 ,  0.49417892, -0.54550925,\n",
       "       -0.39655035, -0.43646888, -0.49091282,  0.46162348, -0.46794618,\n",
       "       -0.44031589,  0.50964374, -0.3913412 , -0.37688475, -0.40508452,\n",
       "       -0.49633748,  0.47433605, -0.49279723, -0.41335784,  0.46424785,\n",
       "       -0.50493388,  0.56208445, -0.47908295, -0.43684264, -0.44836065,\n",
       "        0.59544978, -0.45220765,  0.46646322, -0.38656305, -0.42309473,\n",
       "       -0.46249315, -0.51912213, -0.4717626 ,  0.54899636, -0.44946471,\n",
       "       -0.47066488, -0.51321157, -0.48490824, -0.43693822,  0.54571506,\n",
       "       -0.40529383, -0.62967442, -0.42909576,  0.3990733 , -0.44533584,\n",
       "       -0.43263238, -0.43760169, -0.53502417,  0.41475114, -0.38368674,\n",
       "       -0.48870974, -0.46143669, -0.43526451, -0.47527829,  0.5971817 ,\n",
       "        0.61701783, -0.42556645, -0.39089063, -0.42856923, -0.39387941,\n",
       "       -0.37927151, -0.40875851, -0.49218202, -0.45965644,  0.59230134,\n",
       "        0.57135263,  0.53718798, -0.4345133 , -0.44212475,  0.49821311,\n",
       "        0.44905523,  0.50060636, -0.44373883, -0.4512636 , -0.4391544 ,\n",
       "        0.6228935 , -0.48754293, -0.41194873, -0.62321287,  0.60019337,\n",
       "        0.57312108, -0.42828898, -0.42887221,  0.55499125,  0.47663911,\n",
       "        0.5543774 ,  0.53004613,  0.56805145, -0.53373738, -0.42440086,\n",
       "       -0.45434774, -0.39078849, -0.39893432, -0.39599706, -0.42214797,\n",
       "       -0.55833323, -0.50783044, -0.43336869,  0.44105975, -0.40841155,\n",
       "       -0.4294912 , -0.57912395, -0.40486783, -0.59956929, -0.44749569,\n",
       "       -0.39752298, -0.49517682,  0.53272485, -0.31238054,  0.47515415,\n",
       "        0.4956603 , -0.40564488, -0.3869395 , -0.42560307,  0.40927449,\n",
       "       -0.50037501, -0.55086263, -0.37803959, -0.43195773,  0.60097615,\n",
       "        0.52282709, -0.44418449, -0.45560326, -0.4501239 , -0.42238802,\n",
       "        0.49866655,  0.55484894, -0.46841785, -0.4074456 , -0.46979957,\n",
       "        0.50969412, -0.51583684,  0.64124836, -0.51999819,  0.56073518,\n",
       "       -0.50245946, -0.37555383, -0.42356719, -0.47463957, -0.50398488,\n",
       "        0.48643707,  0.4808579 ,  0.61125837,  0.51632752,  0.56674851,\n",
       "       -0.44435943, -0.54784558,  0.49547007,  0.43823912, -0.58446015,\n",
       "        0.57115184, -0.42753585,  0.57968089,  0.57530058,  0.56934544,\n",
       "       -0.36943333, -0.41572395, -0.43082606, -0.45546178, -0.55257449,\n",
       "       -0.48007001,  0.46967874,  0.46916551, -0.41170669,  0.46880229,\n",
       "       -0.45747685, -0.41940044, -0.60794078,  0.61523607,  0.49464316,\n",
       "        0.50843089,  0.55584684, -0.47676111,  0.5599761 ,  0.48935356,\n",
       "        0.71059915,  0.46113943, -0.57254886, -0.57767836, -0.41417913,\n",
       "       -0.42688085, -0.41397568,  0.55868795, -0.24125274, -0.41171873,\n",
       "        0.61530962,  0.52585687, -0.40504333, -0.44919329, -0.44580855,\n",
       "        0.58270093,  0.49119814,  0.62888422,  0.51990724, -0.41364391,\n",
       "       -0.43671919, -0.42892224,  0.56708931,  0.57206006, -0.41599358,\n",
       "        0.48260478, -0.55499363, -0.26579955, -0.45840988, -0.42637164,\n",
       "       -0.55573258, -0.42912895, -0.42877102, -0.43798925,  0.49366955,\n",
       "        0.58222301, -0.47954145, -0.46073328, -0.35102324,  0.48298283,\n",
       "       -0.45255861,  0.53758593, -0.4445775 , -0.58951583,  0.50988688,\n",
       "       -0.47435238,  0.58538463, -0.40102764, -0.42977687,  0.50730355,\n",
       "        0.50890286, -0.44809533, -0.49920065, -0.42119127, -0.61611846,\n",
       "       -0.42360137,  0.48712021, -0.3988537 , -0.55220445, -0.34985031,\n",
       "        0.5877255 , -0.53213303, -0.4809661 ,  0.45333769,  0.46709127,\n",
       "       -0.54100855, -0.3740966 ,  0.63398706, -0.46987932, -0.49545827,\n",
       "       -0.38735484,  0.60660033,  0.63053168,  0.60238417, -0.59033252,\n",
       "       -0.44451075,  0.59889039, -0.3671292 ,  0.4325701 , -0.57622566,\n",
       "        0.55813869,  0.57543458, -0.51022581,  0.57552239, -0.49215783,\n",
       "       -0.44153652,  0.43058995, -0.39540773,  0.66816592,  0.6071642 ,\n",
       "       -0.55313762, -0.3742192 ,  0.57893714, -0.42214613,  0.50237352,\n",
       "       -0.42699505, -0.45850639,  0.558551  , -0.45292276,  0.40217614,\n",
       "       -0.42629123,  0.53900354,  0.52402019,  0.43546121, -0.43707993,\n",
       "       -0.41194383,  0.59129057, -0.53362708,  0.59167717, -0.52791326,\n",
       "       -0.50593   , -0.44567935,  0.48687908, -0.59018607, -0.42900417,\n",
       "       -0.37671382, -0.49212583,  0.48236739,  0.54238897,  0.49710571,\n",
       "       -0.39273822, -0.44558498, -0.43706533, -0.46672592, -0.58020564,\n",
       "       -0.54909674, -0.40297864, -0.4906701 , -0.45700025,  0.49157513,\n",
       "       -0.44443241, -0.45393898, -0.53312424, -0.41136436, -0.38840603,\n",
       "        0.44007781,  0.5997981 ,  0.3718257 , -0.60278287,  0.63402494,\n",
       "        0.58997812, -0.60153052, -0.59862914,  0.4234275 , -0.41007946,\n",
       "       -0.47169264,  0.52835913, -0.42857567, -0.41789054,  0.5301216 ,\n",
       "        0.73309432, -0.35186878, -0.38749635, -0.4484938 , -0.41600089,\n",
       "        0.43082276, -0.39561662, -0.41110361,  0.55153201, -0.39077488,\n",
       "       -0.39150268, -0.43525202, -0.3524897 , -0.37860875, -0.42811812,\n",
       "       -0.44205933,  0.51451285,  0.4507651 ,  0.47770905, -0.41126686,\n",
       "       -0.4630169 ,  0.56765228, -0.34371343, -0.48011715,  0.57447824,\n",
       "       -0.33009704, -0.4715534 ,  0.56485673, -0.44296318,  0.54708595,\n",
       "        0.52106737, -0.59564151,  0.51112361, -0.54926208,  0.47846634,\n",
       "       -0.41263993,  0.47904985, -0.42645591,  0.52818916,  0.67739139,\n",
       "       -0.45893311, -0.39041187, -0.30354059, -0.41285355,  0.62076577,\n",
       "        0.65592025, -0.38412454,  0.52004425, -0.42568799,  0.54048632,\n",
       "       -0.38194862, -0.40390679, -0.39937667, -0.42587241,  0.53445231,\n",
       "        0.57275726, -0.49860611,  0.54776902, -0.39586484,  0.53421172,\n",
       "       -0.49025071, -0.46543226, -0.39265206, -0.45198757, -0.4280034 ,\n",
       "        0.53319352, -0.57648694, -0.44989575, -0.44030323, -0.43759411,\n",
       "        0.54706185, -0.41544032, -0.44186663,  0.53375457,  0.52660562,\n",
       "        0.70178244, -0.4114453 , -0.41881133,  0.59424535, -0.41591949,\n",
       "       -0.41381167,  0.59325668, -0.38467112, -0.63538197, -0.41700899,\n",
       "        0.40274585, -0.55436528, -0.45442452,  0.48106505, -0.69471746,\n",
       "       -0.56025531, -0.43518304, -0.52263661, -0.52289005, -0.46914438,\n",
       "       -0.39672022, -0.42156556, -0.40114445,  0.41978385, -0.42883724,\n",
       "       -0.42182165, -0.4333079 , -0.41748191, -0.55496414, -0.45609222,\n",
       "       -0.56172886,  0.59378418, -0.47470601, -0.55327947, -0.59736776,\n",
       "        0.57778144, -0.44427846, -0.47506507, -0.39412231,  0.55343975,\n",
       "        0.62355846, -0.32650612, -0.43371086, -0.47352917, -0.61385263,\n",
       "       -0.41151854, -0.50195949, -0.51287605,  0.53928733, -0.51623018,\n",
       "       -0.61477056, -0.47782613, -0.41130042,  0.41141024, -0.4631057 ,\n",
       "       -0.40494968, -0.46686758,  0.47401056, -0.50830185, -0.51571762,\n",
       "       -0.53420383,  0.57193799, -0.37512627, -0.39160045, -0.61802168,\n",
       "        0.38879612, -0.38891255, -0.62446417, -0.41377096, -0.45573211,\n",
       "        0.56188382,  0.45802033, -0.5480608 , -0.58916535, -0.51667516,\n",
       "       -0.45536906, -0.42862773, -0.55193309,  0.46633496, -0.44724617,\n",
       "       -0.44357527, -0.42239169, -0.43630906, -0.3798718 , -0.42439471,\n",
       "       -0.39230997, -0.36762349, -0.41810182, -0.52070314, -0.38117695,\n",
       "        0.52010469, -0.48810046, -0.56945273, -0.35084973,  0.59270009,\n",
       "        0.50089154,  0.59354642,  0.41110009, -0.43689015, -0.43363964,\n",
       "        0.52673569,  0.52816538, -0.44446446, -0.52049812, -0.46050002,\n",
       "       -0.43531339, -0.42222312, -0.61084202, -0.42287447, -0.41583234,\n",
       "       -0.5098017 , -0.46769498, -0.60470189, -0.58908573, -0.53649353,\n",
       "        0.42611078,  0.62988752, -0.41665911, -0.49003706, -0.39557425,\n",
       "       -0.4005905 , -0.40846417, -0.55861593, -0.48027254,  0.56200434,\n",
       "       -0.49208149, -0.40507942, -0.44082319, -0.41562172, -0.32854824,\n",
       "       -0.45682173, -0.48193864,  0.62987348, -0.54040228,  0.38133428,\n",
       "        0.60044012, -0.50654661, -0.65848111, -0.53510846,  0.54647259,\n",
       "       -0.42996072,  0.45676749, -0.48808407,  0.53952472, -0.47112824,\n",
       "        0.45932715, -0.42724392,  0.50902664, -0.35734507, -0.45139185,\n",
       "        0.65792088, -0.48285224, -0.43491883,  0.53007613, -0.43840514,\n",
       "       -0.41700559, -0.53443577, -0.47880955,  0.45993475,  0.46227254,\n",
       "       -0.41685437,  0.68694534, -0.43944118, -0.37735678, -0.41000326,\n",
       "       -0.42860857,  0.55894313,  0.55996263, -0.44074396,  0.43960566,\n",
       "       -0.45353983, -0.47106596, -0.44988052,  0.46726981,  0.5361422 ,\n",
       "       -0.42410754, -0.37641639, -0.43390628, -0.38069292, -0.42553395,\n",
       "       -0.46162766, -0.41037568, -0.38414334, -0.52623675, -0.46515773,\n",
       "        0.51687555, -0.41641856, -0.4296808 , -0.3922523 , -0.53374567,\n",
       "        0.44747924, -0.54946115, -0.39681106,  0.54877127, -0.45156011,\n",
       "       -0.38147369, -0.43445343,  0.44426294, -0.50053613, -0.41351724,\n",
       "       -0.38388644,  0.57104913,  0.60967837,  0.44105715, -0.4458075 ,\n",
       "       -0.43440457, -0.41803058, -0.47280032, -0.4411416 , -0.41000195,\n",
       "        0.64931526, -0.45291344, -0.41125296, -0.57040382,  0.61536931,\n",
       "       -0.59948135,  0.64491092,  0.49365801,  0.48735152,  0.44627328,\n",
       "       -0.41860887,  0.37840821,  0.4181319 , -0.51439108, -0.56298143,\n",
       "       -0.51368589, -0.41254268, -0.55713038, -0.32433721, -0.61044299,\n",
       "        0.52507871,  0.41246085, -0.4180125 ,  0.5479614 , -0.38002978,\n",
       "       -0.44732919,  0.58807273, -0.39661292,  0.57239427, -0.62385278,\n",
       "       -0.38473912, -0.43472867, -0.47408427, -0.37002768,  0.52830643,\n",
       "       -0.47592124,  0.4562679 , -0.38208506,  0.46727911, -0.45084193,\n",
       "        0.54435724,  0.55471298, -0.45884859, -0.39834549, -0.39598097,\n",
       "       -0.39262209,  0.44310162,  0.51045357, -0.52826494, -0.45638478,\n",
       "       -0.49061394,  0.40661917, -0.38686042,  0.44197767,  0.61099521,\n",
       "       -0.36907201, -0.51220133,  0.44778761, -0.36610399, -0.47291948,\n",
       "        0.57652534,  0.60030351, -0.61310854, -0.41170676,  0.43670403,\n",
       "       -0.48427666, -0.38917477,  0.51917814, -0.50180413, -0.48112433,\n",
       "       -0.51321335, -0.38479796, -0.40000648, -0.40760037, -0.44031669,\n",
       "        0.53292549,  0.54362948,  0.62216225, -0.40930927, -0.52626455,\n",
       "       -0.48317898, -0.37473068, -0.54023412, -0.38722996,  0.52537909,\n",
       "        0.46375838, -0.44866414, -0.41569635,  0.48683725, -0.48927533,\n",
       "       -0.57206591,  0.58342737, -0.40179078,  0.56185602,  0.42727353,\n",
       "        0.62189644, -0.43413066, -0.46959532,  0.67030415,  0.44217018,\n",
       "        0.59037177, -0.53017611,  0.48846777, -0.41934578,  0.40663785,\n",
       "       -0.41743882,  0.45166044, -0.53924257, -0.63784569, -0.44638429,\n",
       "       -0.4724989 ,  0.49068197, -0.43675927])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3wc9ZnH8c+zRb1b7rbcu3EB2aZjjAnGQEwNPZQkPrgQSI5LKLnA5XJpRxqEEGJKIAFC6MV0HBvHVNu44W7LsiRcJDf1stp97o9dB1mWViuk3VlJz/v12tfuzI5mvlrMPpr5/eb3E1XFGGOMaY3L6QDGGGPimxUKY4wxYVmhMMYYE5YVCmOMMWFZoTDGGBOWx+kA0ZCbm6tDhw51OoYxxnQZK1eu3KeqvVt6r1sWiqFDh7JixQqnYxhjTJchIjtbe88uPRljjAnLCoUxxpiwrFAYY4wJywqFMcaYsKxQGGOMCcvRXk8i8ihwLlCqqhNbeF+Ae4G5QA1wrap+GtuUsfXy3T/luIbxoDkgB1iZsIF5P/5hj8tgOSxHV8gRDxlikcPpM4rHgDlh3j8bGBV6zAf+GINMjnn57p9yXP100FzABZrLcfXTefnun/aoDJbDcnSFHPGQIVY5xOlhxkVkKLCwlTOKPwFLVPVvoeXNwExV3R1un/n5+doV76MouePF0H/s5qopStoQkwx5deOBVEczWA7L0RVyxEOGsDlkH4N+fkHE+xGRlaqa39J78X7D3UCguMlySWjdUYVCROYTPOsgLy8vJuE6nea08kYKeXXHxSiExEEGy2E5ukKOeMgQJker3yftF++FoqVPoMVTIFVdACyA4BlFNENFjRxo+YxC9pN21rExiVD11qeOZ2gzx1dimONty2E52p8hNYafRXWrOQ502jHivVCUAIObLA8CdjmUJarueOCXzHcF8PpPbvZOXbBhambkp5Ad8d7iJ4LXO0lyLEObOU6PYY4llsNyxG8GgKVhcgyic3I43ZjdlleAr0vQ8UB5W+0TXdGyvfuZUFRAkn80da6DIPuAAMg+ViZ+wuzzvxWzLLPP/xYrEz9xNIPlsBxdIUc8ZIhVDkcbs0Xkb8BMIBfYC9wNeAFU9cFQ99j7CfaMqgGuU9U2W6m7UmP2rroG/nLHt7kurRGf73qeS1jNpSkz8B+qx52VSMZZQ0md2iemmapXlVLxVqGjGSyH5egKOeIhQ2flCNeY7Xivp2joKoWi1h9g3l8e4n+2vkhe4AYKE71kXjKICRMnOx3NGNPDhCsU8X7pqdsKqHLxeys4f+12Jrj6EKAXH7i2WJEwxsQdKxQOuW39Dk5843EuSH+NQ/5L2ZpYxaxzTnI6ljHGHMUKhQMeLylj5drFnKQHSA0cj2gGHyVu5Nj86U5HM8aYo1ihiLEPD1byo03bOXvFHk5MXcr+wKVsTqzglFNi1+/aGGPawwpFDO2srefKVVu56Y2/clnSsxwMXIQ3kMyytK2cfMpMp+MZY0yLrFDESLmvkQtWbmH6zjcZo14Gesuo8l/AZu9BZkwZ6XQ8Y4xplRWKGPAFlKvWFlBee4Dxa5M4K+0pNnhuxBtw84+cIuacea7TEY0xplXxPoRHl6eq3La5mOUVNdzx2rOcnbgJlTSSak9li2cf04b1djqiMcaEZWcUUfZQSRlP7TnAv6+5jxxXH0YnL2dp6o9ICMBbffdw/vmXOh3RGGPCskIRRe/sK+fubbs4oXgJ9TuncUHKI2zzjGfkoZFs9pQypVdLY9kbY0x8sUIRJRuqavnW+kIG7CvhKysKucizlBT3Id5P+xGegPJ6/31cecU1Tsc0xpg2WaGIgtJ6H1es2Y6rppqbt/6RgHs4U1Pe4PmsSzhlbxqb3LsZn9zapCfGGBNfrFB0slp/gK+vK6CsroGb1v2K9WUXc2nKAg5IGmXua3GpsnDgIb553XynoxpjTESsUHSigCq3bNzJ6soa5r/9IJlFIznPtYJcbzG/GHI7c3cF2ODexUhtIDiCujHGxD8rFJ3onh17eKWsnHOXvc2klBL2u8dyQtrfed8zgRGVx6EoCwdW8p0b/93pqMYYEzErFJ3kuT0H+O3OvYzdsY1T017lg/LLuTzlzzQq/GLM3czd5WO953OG1FXjcrmdjmuMMRFztFCIyBwR2Swi20Tk9hbezxSRV0VkjYisF5HrnMjZlk8OVfG9TcXkVFXy7dX3oFtnMks3MzhxLQ9kX8gFJWk0EuDVgVX85y03OR3XGGPaxbFCISJu4A/A2cB44HIRGd9ss28DG1R1MsEpU38tIgkxDdqGnbX1XPvZDjz+Rv734R+RmtmbHZ6JzEx/jILGPrwwfD6z9/r5zFPCgKpK3G6v05GNMaZdnDyjmA5sU9UCVW0AngbmNdtGgfTQ3NlpwAGgMbYxW1fR6OeqtQVUNfr5wUO/pXZOgCXVV3JZ8t9IkGpuG/Fd/n1LDT78vDqwljtuucHpyMYY025OFoqBQHGT5ZLQuqbuB8YBu4B1wC2qGmhpZyIyX0RWiMiKsrKyaOQ9QmNAmf9ZIdtr6vnqOwvJmrgFWfsVpvl3MTZ5Ca8HplKaczyn7hfWekroXV5FQoLdiW2M6XqcLBQt9Q/VZstnAauBAcAU4H4RyWhpZ6q6QFXzVTW/d+/oDrSnqvxwawlLDlYyrmAL52x9gb7V/VjnncCZ6X9iny+NO6d+n++uL6cOHwsH1vGjm+0ubGNM1+RkoSgBBjdZHkTwzKGp64AXNGgbsAMYG6N8rXrk8308vms/faqruPvR/6N8ZiZv1V/FxQkvk+XZywNJ59JX+jCtwssabxE5h6pJTslxOrYxxnwpThaK5cAoERkWaqC+DHil2TZFwBkAItIXGAMUxDRlM+/ur+CurZ+TLfCz3/6YoivdyLq5jPdVMDX1ZVZW5/FI/rV8f81+amngtf4N3HXjJU5GNsaYDmmzUIjICBFJDL2eKSI3i0hWRw+sqo3ATcBbwEbgGVVdLyI3iMjhVt+fACeKyDpgEXCbqu7r6LG/rI1VtdywvpBMj5tvPfZH6iceZOiOwSz3jmNOxgPUBhK4p//VTKj2MKEumVWeItIP1ZGe1bzpxRhjuo5IJi56HsgXkZHAIwT/6n8KmNvRg6vq68DrzdY92OT1LuArHT1OZyhr8HHV2gJcAvn/XMzk3cspz8/i1R2XMc+ziL7e7fzt4DSWnjqLp5eUUk0Crw/w8ZuLznY6ujHGdEgkl54Cob/+LwB+p6rfA/pHN1Z8qfUHuGbdDvY1NNKruIhvP/coe67yElh/IUPrGpmR9iTbavvyx4nXcEJZA8P96azyFpJa3kBO31FOxzfGmA6JpFD4RORy4BpgYWhdj7lrTFX57qYiPq2oIcvXwM8fuIedX0tg5Kcjed87jDnpCxAJ8FT9DLaMmMhta/ZTRR1v9g/woytOdDq+McZ0WCSF4jrgBOCnqrpDRIYBT0Q3Vvy4p3APL5ceYnCCh+8+dC8Vw2pJS0jnRddFnCWfMixpJUv2D+f5Ey5lTmE5AySLld5Ckir89BtyrNPxjTGmw9pso1DVDcDNTZZ3AL+IZqh48fyeA/ymcC+jUhI59qVnGbFrPSU3JpL0/hX0x8XJvR5ir683z2XMZG/vgXxveTEV3iTe7hfgxzPHOR3fGGM6RSS9nk4SkXdEZIuIFIjIDhFxtItqLCwvr+Z7m4oZkZxI4prVXLPwWXZf62LMh5NZ6unPrLQnSHVVsHDPcBYdfw6XbtxNL082Kz0FeKuEoRNmOv0rGGNMp4ik19MjwPeAlYA/unHiw87aeq5dt4PcBA8VZWX8+tH72X5uOr32p/Fs4tmc2lDAMSnv8FHFKN4ecTrVaRncsP0gBxOrebcv3DGjR7X1G2O6uUgKRbmqvhH1JA57fs8Bfl6wm8/rfbgl+MF43MLdj/6eA/38NE4Uqj66ml6NiZza6w9UB3JYfCCPD86axY3Lt5Ge0I9Fns+Q2gTGTjvP6V/HGGM6TSSN2YtF5B4ROUFEjj38iHqyGHp+zwH+c3MxJfU+FGhUqFcY98mHDCzaTOVlDYxfdjyLXb04NeVFcj27WLhnGEvyz0JdHi7f7WW/VLG4r3BLfpLTv44xxnSqSM4oZoSe85usU2BW58dxxs8LdlMbOHI8QgU2DBlB0ZXJDNzSi6dTT2dGbRnHZT3P5oZxrGUwa8dN484la0lOGsEyz1r89SlMmXmFM7+EMcZESSS9nk6PRRAnfV7va3F9aU4uruoEDhReR6YviZOzHwASebs4l3fPPI/k+gbmVmZTllzJe31c3DisgeDUGcYY031E0uspU0R+c3iuBxH5tYhkxiJcrGR5Wp7D+iT/Ysa+P5N3JZXpSf8gL2ETbx06hu29hrJz8Ej+Z9FKEpNy+NSzHV9jGiec880YJzfGmOiLpI3iUaAS+FroUQH8OZqhYk4Vl//IDl05gVJ+sHQpT2VOZ6qvhhPTH2NfYAzr9yby5qnn0+fAQU7y57HHVc6y3m6u77+P4OyuxhjTvURSKEao6t2hKUsLVPXHwPBoB4ulYz9cyq1PLqDv/jJEA2TX7uOunb+irPJa0quSmZHxEAnSyPOf92PjqEkcys7lZ++sxJuUxafu7dRqBjMvudHpX8MYY6IiksbsWhE5WVWXQfAGPKA2urFi64ZX/k79xHJ+6VmCRxvZ35jJMR9dygPJCcxJWMn4pI/4pOEMDtYG+McJcxmzs4iJyePY5TrER7kerkwtxu223k7GmO4pkkJxI/B4qF1CgAPAtdEMFWt1x5TDhdV4PaAKY94fyROZE5lUpZyS+wB1gTyWFgb48LjTqU9O4SdL1uLpdxqfelZS5erLnK9/3elfwRhjoiaSXk+rgcmH56pW1Yqop4qxwLk1pK50kfmqG091OgVnXENqXSJT0x8iw1XOkwdPodbbwIpJJ3HKmtUMyTmOYvcBlvfycilb8CakO/0rGGNM1LRaKETkKlV9QkT+o9l6AFT1Nx09uIjMAe4F3MDDqnrUYIMiMhP4HcGhzfep6mkdPW5ziWu89Fk6jaT8C2lIzub1umpOc28jP/lNivVMdpdV8tbsSwm4XNz+8Q7cQ0bwqXs55e4BnPONSzs7jjHGxJVwZxSpoeeW/lzWFta1iwS7CP0BOBMoAZaLyCuh0WoPb5MFPADMUdUiEenT0eO2pN/H01g87ioO1btIbWhkIh5O7XU/fs3h+SIP+zPT2TZsLBe99x65A06k0L2P1TleLqjZQFLyZdGIZIwxcaPVQqGqfwq9fFdV32/6XqhBu6OmA9tUtSC0z6eBecCGJttcAbygqkWhTKWdcNyjLBlyGXV1btIIni1NTn2Vft5inqu8Fn/tdhaecw0JDT5u3HgA9/AUVrnXsT9xMOd/4/xoxDHGmLgSSffY30e4rr0GAsVNlktC65oaDWSLyBIRWSkirbYai8j8wzcFlpWVtSvIIX8C3lCRyHDvZlra02yvm07R7hq2DRnHvtz+fPP1hWQMPoXtnlLWZSdwzr51pKYPbtdxjDGmKwrXRnECcCLQu1k7RQbBNoWOammsi+aXtDzAccAZQDLwoYh8pKpbjvpB1QXAAoD8/Px2XRpLCwijkt7j+LQnSXeXAUJxTT8C/j28fdrXyTxUztd2CTIyidWuNZQlD+HCq7/SnkMYY0yXFe6MIgFII/hlnd7kUQFc3AnHLgGa/kk+CNjVwjZvqmq1qu4DlgKTO+HYRxicvITTM/5IhqcMERBRTspaSNq4bGpS0rn1xWdIGXoqW72lbMxKYPaudWTnju3sGMYYE5fCtVG8B7wnIo+p6s4oHHs5MCo0B/fnwGUE2ySaehm4X0Q8BAvXDOC3nR3ktLS/4HXVH7HO6wpwbsKHPFLyOTNr+yAeL2ukgD2pw/jJ5Sd3dgRjjIlbkdxwVyMi9wATgH/dfqyqHRpmXFUbReQm4C2Cl7IeVdX1InJD6P0HVXWjiLwJrAUCBLvQftaR47Yk03WoxfUDGsq487mnSB59PZsS9rI1M4GZhevo3f+Szo5gjDFxK5JC8STwd+Bc4AbgGqB9rcWtUNXXgdebrXuw2fI9wD2dcbzWVLn6kN5Ch6oyVxYTksahbjdrKODz9BH88MJOv/JljDFxLZJeT71U9RHAp6rvqer1wPFRzhVTr9Ycgy9w5EdR40qkoGAUyUNOZGPCHnZkJHLytvUMHNbtp+cwxpgjRFIoDs/qs1tEzhGRqQQbnruNz3f6eXv3KMobElGF4sS+3J80j8ak81CXsFZ2UJw5iIuuPc7pqMYYE3ORXHr639CAgLcSvH8iA/heVFPFmKifF/rM5nszzqQiLQtQLl3yNkMyJ/FZ4i6K0hKZsXkzw775Q6ejGmNMzEUyKODC0MtyoFted1k5cQb/nHEWjd6E0BphfM4p+Pc0so5CCrNHcePMepvm1BjTI0UyFerjoTGXDi9ni8ij0Y0VW8umndmkSMCQKj9z9gR4fpCbz1MTyd+4nXH5zXvuGmNMzxBJG8UkVf1X/1FVPQhMjV6k2PMlJB6xPH97A/VueHhkCjty8vjqvExEIvmojDGm+4nk288lItmHF0Qkh8jaNrqMpKovJuwbUennrD2NPD0kAV+gjqmbdjL55G86mM4YY5wVyRf+r4EPROS50PIlwE+jFyn25r/8NAWnXsa/7QjQr04JAHsTAly4+E3mnpGBy+V1OqIxxjgmksbsv4jICmAWwYH8Lmw6Z0R3cHFhOQm96vG4gh+HALduasC3aicjfv5HZ8MZY4zDwo0em6GqFaFLTXuAp5q8l6OqB2IRMBaSJpyPy3XkR5GAC8/48/F4UhxKZYwx8SHcGcVTBIftWMmRw39LaHl4FHPFlCT3atd6Y4zpScIVisPzV49T1bpYhHFKfUI5Sb6sFtcbY0xPF67X072h5w9iEcRJ73uK8OE/Yp0PP+97ihxKZIwx8SPcGYVPRP4MDBKR+5q/qao3Ry9WbBVoGeLZyLTGEaSRRBV1LPdsp0A7ZZBcY4zp0sIVinOB2QR7O62MTRxnVCUmU8BeCjx7j1pvjDE9XbgZ7vYBT4vIRlVdE8NMMffpoDGcuGMd3sAXl598LjefDhrjYCpjjIkPrbZRiMgPQi+/KSL3NX90xsFFZI6IbBaRbSJye5jtpomIX0Q6Y67uo4wpSuGfIydTmZiMApWJyfxz5GTGFFnXWGOMCXfpaWPoeUU0DiwibuAPwJlACbBcRF5pfjNfaLtfEpwyNSpO8b4KOy5m4eTZlKe4yKwJcPqaKk7xPgd8JVqHNcaYLiHcpadXQ8+PH14nwZHx0lS1ohOOPR3YpqoFoX0/DcwDmt/1/R3geWBaJxyzRbO+PpvAn59k4qtnE5xuo4LMoW8w6+vnROuQxhjTZbQ5hIeIPEVwrmw/wUbtTBH5TWgu644YCBQ3WS4BZjQ79kDgAoIN6mELhYjMB+YD5OXltStI/37zmH0dFGz/FXX1u0lK7M/wEf9J/37z2rUfY4zpjiIZFHB8aCiPK4HXgdsIFoyOFoqWZgHSZsu/A25TVX9bkwap6gJgAUB+fn7z/bSpf795VhiMMaYFkRQKr4h4gfOB+1XVJyLt/iJuQQkwuMnyIGBXs23yCfa8AsgF5opIo6q+1AnHN8YYE4FICsWfgEJgDbBURIYAndFGsRwYJSLDgM+By4AjppFT1WGHX4vIY8BCKxLGGBNbkQwzfh/QtDvsThHp8NzZqtooIjcR7M3kBh5V1fUickPo/Qc7egxjjDEdF0lj9i3An4FK4GGC06DeDrzd0YOr6usE2z2armuxQKjqtR09njHGmPaLZCrU60PdYb8C9Aau44uRZY0xxnRzkRSKw92N5gJ/Dg3nEb4LkjHGmG4jkkKxUkTeJlgo3hKRdCAQ3VjGGGPiRSS9nr4BTAEKVLVGRHoRvPxkjDGmB4ik11NARHYAo0UkKQaZjDHGxJFIej19E7iF4A1xq4HjgQ8JDqthjDGmm4ukjeIWguMs7VTV0wl2j7Wp34wxpoeIpFDUqWodgIgkquomwGb0McaYHiKSxuwSEckCXgLeEZGDHD0mkzHGmG4qksbsC0Iv/1tEFgOZwJtRTWWMMSZutFooRCSnhdXrQs9pwIGoJDLGGBNXwp1RrCQ4P0TTu7APLyswPIq5jDHGxIlwU6EOa+09Y4wxPUebvZ5E5AIRyWyynCUi50c3ljHGmHgRSffYu1W1/PCCqh4C7o5eJGOMMfEkkkLR0jaRdKs1xhjTDURSKFaIyG9EZISIDBeR3xJs6O4wEZkjIptFZJuI3N7C+1eKyNrQ4wMRmdwZxzXGGBO5SArFd4AG4O/As0Ad8O2OHlhE3MAfgLOB8cDlIjK+2WY7gNNUdRLwE2BBR49rjDGmfSK54a6a4NSnh7/cU0PrOmo6sE1VC0L7fhqYB2xocuwPmmz/EcGBCY0xxsRQJL2enhKRDBFJBdYDm0Xk+51w7IFAcZPlktC61nwDeCNMzvkiskJEVpSV2ZiFxhjTWSK59DQ+NGf2+cDrQB5wdSccu6XpVLXFDUVOJ1gobmttZ6q6QFXzVTW/d+/enRDPGGMMRFYovCLiJVgoXlZVH618obdTCTC4yfIgWhhsUEQmAQ8D81R1fycc1xhjTDtEUij+BBQCqcBSERkCVHTCsZcDo0RkmIgkAJcBrzTdQETygBeAq1V1Sycc0xhjTDtF0ph9H3Bfk1U7Q5eCOkRVG0XkJuAtwA08qqrrReSG0PsPAncBvYAHRASgUVXzO3psY4wxkRPVlq8iichVqvqEiPxHS++r6m+imqwD8vPzdcWKFU7HMMaYLkNEVrb2h3i4M4rU0HN650cyxhjTVYQbPfZPoecfxy6OMcaYeNNmG4WIDCN4d/bQptur6lejF8sYY0y8iGRwv5eAR4BXgUB04xhjjIk3kRSKulDPJ2OMMT1QJIXiXhG5G3gbqD+8UlU/jVoqY4wxcSOSQnEMwSE7ZvHFpScNLRtjjOnmIikUFwDDVbUh2mGMMcbEn0iG8FgDZEU7iDHGmPgUyRlFX2CTiCznyDYK6x5rjDE9QCSF4u6opzDGGBO3IhkU8L1YBDHGGBOfImmjMMYY04NZoTDGGBNWq4VCRBaFnn8ZuzjGGGPiTbg2iv4ichrwVRF5mmZzXNud2cYY0zOEKxR3AbcTnMu6+SRFnXJntojMAe4lOMPdw6r6i2bvS+j9uUANcG13L1A/u+kmfJkZqMeLNPrwlldw5/3397gMlsNydIUc8ZAhFjlaneHuXxuI/EhVf9JpR/xiv25gC3AmUEJwDu3LVXVDk23mEhzifC4wA7hXVWe0te+uOsPdz266iYacHHA1uSIYCJBw4EDM/vHFQwbLYTm6Qo54yNCZOcLNcNdmoQjt4KvAqaHFJaq6MOKjt77PE4D/VtWzQst3AKjqz5ts86fQ8f4WWt4MzFTV3eH23VULxY9/eCfqTTj6jUAAV0NsRlAJJCQc+Q/OgQyWw3J0hRzxkCFcDvE1cPdPfxbxfr7sVKiHf/jnwHTgydCqW0TkJFW9I+IELRsIFDdZLiF41tDWNgOBowqFiMwH5gPk5eV1MJoz1ONt+Q0RXI2xmQokkCgtvxHDDJbDcnSFHPGQIVyOVr9PvoRI7sw+B5iiqgEAEXkcWAV0tFC09Ns1P72JZJvgStUFwAIInlF0LJozpLER9R79H1cafdQfMzQmGTxbSlo8q5FGH3XHDItJBgDvlmLL0cVy1E4aGrMcCZtb/3caqxzxkKGtHJ0lkkIBwUEBD4ReZ3bSsUuAwU2WBwG7vsQ23cLPr7sO+vQGjwekSX0MBPCWVzB99biY5Fjl29Di9c5ghrExyRDMsb7VHDNimONTyxFxjuNXxebfKMDKMP9OY5UjXIYTYvhZrAiTo7NEUih+DqwSkcUE/8I/lY6fTUCw8XpUaE7uz4HLgCuabfMKcFOoe+4MoLyt9omuqGjNRvrU+ihOTsFdWUsgyX1E74XSzDwK9sfmJKm0Vx59DhQd1YMilhnayrHdcsRljm09LEe4DFvj4rMY3PYPRyiSsZ7+JiJLgGkEC8VtqrqnowdW1UYRuQl4i2D32EdVdb2I3BB6/0HgdYI9nrYR7B57XUePG2/U52f9bf/F51NGIr5GUvrs4rmsnVQlHiStPpsZRecy+eAYvpZ3a0zyPFPy36zJho/zFjqWoa0cl+R9P2Y5ni25q8Uckw6O4WtDfhCzHM8U/4i1rea4LYY5/qvVHJcM6Yy/HyPzbPGdreQYyyVD7nQ8w8VD/ysmGQCeK7q91RydJaJeT11NV+r19PD1N1Hvr6FsWB5QSa89c3Dh/tf7bnycnvF7xpw2JiZ5Nr+3mcUV38HPF20lsc7QZo5TR8cux9ItYXKMimGOra3nOGVk7HL8c1uYHCNimGN76zlOHh6bDMsKwmSIXbvR5mU7WslxP2P+79WI99OhXk8mej7+6yuM2LSGpaefgquukbTcAwxL+JT9jUOoCuSS5trHCWlPMLp/IZzT4R7JERmzZSLC7/mw6irHMrSZ49wY5tjaBXKcF8Mc27pAjq/G6P+V7c5naDNHJ7FC4ZDqnYfwL3iQFZMnot4EUrN3kVswhzm9v4NH6r/Y0JsMZ9wXu2Bn3MXoV29mdMo/nctgOSxHV8gRDxlilCPs6LEi4hKRzzrtaAaAQG0jr3z/VvZ56qjunYvUKoif0zIfwDPrPyFzMCDB5/Pug0lfi124SV8LHtPJDJbDcnSFHPGQIUY5IhnC40ngDlUt6rSjRlk8t1FoQFl41/3kvfwwC884DV9GNgMHr2XoZ9M5c045nP2LtndijDGdrKNtFP2B9SLyCVB9eKXNmf3llLyxgQFvP8uno4bjy8pBGoSayt6cOPBFmLXI6XjGGHOUSArFj6Oeooeo236IVX/4H/rVH6J45DSkURk3ZTHjPhlG6jf/GxLTnY5ojDFHaXOGu9Cc2YWAN/R6OdCth/qOBn9FPS/+6neM2LGOpcdNIZCSToJXKd09hon5jTB2rtMRjTGmRW0WChH5FvAc8KfQqoHAS9EM1d2oP8CaBxcz/sPXKO6TS3W/AbgahHHHvsYFpbuQuf/ndERjjGlVJHNmfxs4CagAUNWtQJ9ohupuDrKMi80AABRqSURBVLxewIHX7sVdX8OKiWPQhCSyeh1iT+FxDJw3DzIGOB3RGGNaFUmhqFfVfw2uLiIeWhnB1Ryt9rN9vPn33zNwbyHvTxlLY05fXPUJDJ2wiItq6iD/eqcjGmNMWJEUivdE5E4gWUTOBJ4FIr8vvAfzldXw3uOvM3n1Eqp751HWpy+43AwdtoG926bR+5ofgMvd9o6MMcZBkRSK24EyYB3wbwQH6ovdiFddVKDBT+Fja0l/7yHcCq+Ny6YxqzfuhlSyh67mPNcg6DfR6ZjGGNOmSEaPDYQmK/qY4CWnzdodRxLsRKrKwRe3suaff2RCxT62H3sarvQk/CpMmLyI0m359L0+diOPGmNMR0TS6+kcYDtwH3A/sE1Ezo52sK6s+pM9/GPpm4zfuhz6T2ZlegX+9CwSNYnEXiWclTADSUp1OqYxxkQkkhvufg2crqrbAERkBPAa8EY0g3VVDSWVbHp+NcPefwJJSOe18WlIeg40upk87UX2bj+Wgd/8N6djGmNMxCJpoyg9XCRCCoDSKOXp0gI1Psr+uoHSFQ+SWldN7dQLaPA3EkhKISOhEUmtZKZ7FuKJ5GM3xpj40Oo3lohcKCIXEhzn6XURuVZEriHY42l5Rw4qIjki8o6IbA09Z7ewzWARWSwiG0VkvYjc0pFjRpsGlAN/38x7m15l2K4teEfM5rmU5fhyB+JuSGb8tOcpK5jCkKuvcjqqMca0S7g/bc8LPZKAvcBpwEyCPaCO+mJvp9uBRao6ClgUWm6uEbhVVccBxwPfFpHxHTxu1FQuKebTdeuZsPIVJH0AK0f1Ic2Vi3q89MrehXr9HN84C1eSt+2dGWNMHGm1jUJVozk/9TyCRQfgcWAJcMTEv6q6G9gdel0pIhsJDh+yIYq5vpS6bQfZ/eYOEj/4I95AAPdxX2dD1UvIgGPw1mUy6qSn2Fs4hTOvuMLpqMYY025tNmaLyDDgO8DQptt3cJjxvqFCgKruFpGwQ4KIyFBgKsEuuq1tMx+YD5CXl9eBaO3jL69n/1ObWLvtaSYe2kviMZfyVNJSvDKIRlzkDFpBQIRjK0/HnZUSs1zGGNNZIun19BLwCMG2iUCkOxaRd4F+Lbz1w0j3EdpPGvA88F1VrWhtO1VdACyA4MRF7TnGl6WNAfY/uZEPy9YxbuNSXL3HUZQ3FNfBDTTkDSapNosRYz+itHgKsy6/PBaRjDGm00VSKOpUtd2Tr6rq7NbeE5G9ItI/dDbRn1Z6UYmIl2CReFJVX2hvhmgrf2MHRdtLGbzsYVyeJDzHXs0bdY+TmjsS8XvIGPsWiJtJpbPw9styOq4xxnwpkfTTvFdE7haRE0Tk2MOPDh73FeCa0OtrgJebbyAiQvBMZqOq/qaDx+t0NWvLKF/2OXvWPkJGbSUpx17Lu96VZNenE0hJI7UhkVF5W9lXMo6xF1/odFxjjPnSIjmjOAa4GpjFF5eeNLT8Zf0CeEZEvgEUAZcAiMgA4GFVnUtwaPOrgXUisjr0c3eq6usdOG6n8JXWcPC5rbx/YBlTitfiHnIye/sNoKTsLTz9J+H2pZA0+QVAGLvrDBJG9Hc6sjHGfGmRFIoLgOFNhxrvKFXdD5zRwvpdwNzQ62WAdNYxO0ugwc/+Jzeysb6McR/9HU3JwX3MhSxseJrkpH40eBNIbqhhdN997Ns1mlPmnud0ZGOM6ZBILj2tAewCO6HB/l7YSvnuKlzLHiCx0Ufa9Bv5xL2epP0HacgdgLcui8T8FxGU4cVnkjhpqNOxjTGmQyI5o+gLbBKR5UD94ZUd7B7bJVV/tJuaVaWs3vkKk/YX4Rk3jz1Z6ayuep707EEgLjSxmNGZtRzYM5ITTz2TYFOLMcZ0XZEUirujnqILaCiu5NDCApZLIePXvYk/ewje0bNZEniNXhVuaob1Jrm2FyknP4NH/AwuOouUy+L2RnJjjIlYJPNRvBeLIPHMX+1j/5MbKU1U+r3yICJuMqbdwCeymap9W/D0HY2oh9KsVcxODbB/7zDmTDkZcdvZhDGm64tkPopKEakIPepExC8ird741t0cHuyvvqKe4g8fJrvqIElTrmZXiotV9StI00w0NZO02gyGT/qABPHRr/Bs0s7oaA9iY4yJD20WClVNV9WM0CMJuIjgBEY9QuU/iqjfcpAPqlYyascnNA6YhAw+jo9cH5BStpfafkNwNSaxvv9ihia7OFiWx6SRM3Al2FzYxpjuod0TI6jqS3TsHoouo27LQSoWFbEtt4ExSx6nMTGFrKnXs0oK2FuxDUntAwmJpDe4yR+7hSSpp9eOs0mfO8Pp6MYY02kiGRSw6W3FLiCf4A133VrjoToOPL2JupwkfK/cQ3JDHYkn/wcl3gY2BD4je38D1SPz8DZk8MGwV/l6ootD+wYys99xuFNtKHFjTPcRSa+npneMNQKFBIcJ77a0McCBJzcRaAzwyZqXmFq6Fd/I00jMHcEKzyJ8e7ZBn4GIy0Wir4pZQ/aRIrWk7DyLzG+c6HR8Y4zpVJH0eormvBRx6dBrBTQUV7Iip5SJq16mLr0XueMv5cNACaU1ZWTUJVA1sD/Jdb1YNuYxrk90UXGwLyemHosnJ9np+MYY06laLRQicleYn1NV/UkU8jimelUpFW8V4j8UvKdwf78Ecp+6D5cqadNvpliq2e5ZTdKuEqr6j0LURXViEWf395MmVfgLLyT7UjubMMZ0P+Eas6tbeAB8g2az0XV11atKOfTC1n8VCZ8qBe88Rq+KUvwTv0ogPZcVnlVUlX+Ox5OOpGaTVpvL+tFv0ifBRVV5LhM5joRBmQ7/JsYY0/nCTYX668OvRSQduAW4Dnga+HVrP9cVVbxViPqCA+OqKu8f2MjUgveo7D2Y/sPO5v3GPRzkEJn7KqkcMRGvP4Ht2av4Sm4imXKQhsLLyDl3usO/hTHGREfYNgoRyQH+A7iS4NzWx6rqwVgEiyX/oXqK6/1srAtQ39hA/vKH8Hk89M6/mSLKKUz5AP+uAuqzcnB7k8moymDThH+S482gujKbqTUzSBzd2+lfwxhjoiJcG8U9wIUEpxc9RlWrYpYqxopdPnZue4nJhUtI9vkQoHHYafgTUvlY1lBbXU9GFVSMGkqCL41lAxZzRk4OvWQvdYUX0WvOFBv8zxjTbYU7o7iV4Gix/wX8sMkXoRBszM6IcraY2bnhOSZsW4JHv7g9xFP0IQVJmdRMDOD9fCfl/frhdrtJq3OROXg9We4caqoymXjwFJKnDnYwvTHGRFerjdmq6lLV5GZDeGQcXu7IQUUkR0TeEZGtoefsMNu6RWSViCzsyDHDGbV92RFFAgB/A722vUHDwV0k+hNwZeaRUJfDwmGvMTWzH31kLxSdRu/TxyAuO5swxnRf7R7Co5PcDixS1VHAotBya24BNkYzTLLP1+L6gEDavkMcyBuIILilgrF9S8hwKXU1aYzfO4vUE0dGM5oxxjjOqUIxj2DjOKHn81vaSEQGAecAD0czjCQfPYGfAp/l9cGXnERCYm9SavqwcMSrjMwYykBXCf6iU+h9whDE69RHaIwxseHUt1xfVd0NEHru08p2vwN+AATa2qGIzBeRFSKyoqysrF1hEsddCO6EI9aVZmWwPzWJyoEjcKmHQyk7OKHXITLET31tCmM+/wppM8e16zjGGNMVRa1QiMi7IvJZC4+IxokSkXOBUlVdGcn2qrpAVfNVNb937/Z1Va3Pm0LilKuR5BwAAik5rBrWh/LcVJI8KaRX92bpsLfpkz6WIa4dNBSdSN8p/XGl2OB/xpjuL5JBAb8UVZ3d2nsisldE+qvqbhHpD5S2sNlJwFdFZC6QBGSIyBOqelVnZ13XUM20wfmkDQ4OD/5hxVv496/B32s0CY3JbMpZyazsOtLx0VCfxOiSuaRdMamzYxhjTFxy6tLTK8A1odfXAC8330BV71DVQao6FLgM+Ec0igRAtWc577vXU0ktlb6DFB5czd68LBJcbjJrMtiS9wHJaZMZ6dpMbdF0+o3KxZOZGI0oxhgTd6J2RtGGXwDPiMg3gCLgEgARGQA8rKpzYxlmf0oddeWb2F26CGlsAJeL5OTheOuzWDxwEWdl+UkP1OHzJzKyeB7pN0+NZTxjjHGUI4VCVfcDZ7SwfhdwVJFQ1SXAkmjlqTtUT8LeQlyhJvP63AG4BJIryqg5ZiMNKdMZJ8s4tPMEBgzIwtsnJVpRjDEm7jh1RhFXZP8O3IHgTXMBbyK+nD54y/dRXbmDORkKgRoaA16G77yA9OvtbMIY07NYoQASG6AhI4eG3gNRb7CbrNRWk1wvlCafwmx5l0PFxzE1K4vEYTaUuDGmZ7FCAVT1yoLcIeBy/2tdQ9/BNLh9ZPur8ONhSOFFpF9qPZ2MMT2P3VYMNOYOPaJIAOBy05g7lKnuj6nYNZ5BSb1IGpfrSD5jjHGSFQrAIy3fOOcRLwRcDN5xEemzx9rgf8aYHskKBeDyt3xPhMufSPnuMQySfqQcOyDGqYwxJj5YoQDSqvtCoNlHEXCR3pBL/x0XknHqUMRjH5Uxpmeybz9gfa91pFcMx9WYCAquxkTSq4YheR8xxDeE1JOGOh3RGGMcY72egC1D3gVg0u7ZuP1JSGIF/SY/w+NJa7luyK24Eu1jMsb0XPYNCJyTWc8zee/wyeB3SXMpd/WvZW2tmxMV0mbaxETGmJ7NLj0Bw1OyuTS7gWx3gJnpPrwCiaIMT8nGnZ7Q9g6MMaYbs0IBPO26gsmpcPeAOmZnNCICY1OEp11XOB3NGGMcZ4UCSK4LAM3vkZDQemOM6dmsUABfk7/ipfGIdV4a+Zr81aFExhgTP6xQAKkJ5e1ab4wxPYkVCqChPrVd640xpidxpFCISI6IvCMiW0PP2a1slyUiz4nIJhHZKCInRCNP8tYz8PuPHBTQ73eTvPWouZWMMabHceqM4nZgkaqOAhaFlltyL/Cmqo4FJgMboxFm1O4L8Gy4iIa6NFShoS4Nz4aLGLX7gmgczhhjuhSnbribB8wMvX6c4DSntzXdQEQygFOBawFUtQFoiEYYd4qf0bvnwu65zdY3tvITxhjTczh1RtFXVXcDhJ77tLDNcKAM+LOIrBKRh0Wk1UYDEZkvIitEZEVZWVm7wmScNwFxH9kVVtwBMs6b0K79GGNMdxS1QiEi74rIZy085kW4Cw9wLPBHVZ0KVNP6JSpUdYGq5qtqfu/evduVNXVqH7IuHoc7KzjcuDsrkayLx5E6taX6ZYwxPUvULj2p6uzW3hORvSLSX1V3i0h/oLSFzUqAElX9OLT8HGEKRUelTu1jhcEYY1rg1KWnV4BrQq+vAV5uvoGq7gGKRWRMaNUZwIbYxDPGGHOYU4XiF8CZIrIVODO0jIgMEJHXm2z3HeBJEVkLTAF+FvOkxhjTwznS60lV9xM8Q2i+fhcwt8nyaiA/htGMMcY0Y3dmG2OMCcsKhTHGmLBEVZ3O0OlEpAzY6XSODsoF9jkdIk7YZ3Ek+zyOZJ/HFzryWQxR1RbvLeiWhaI7EJEVqmrtM9hn0Zx9Hkeyz+ML0fos7NKTMcaYsKxQGGOMCcsKRfxa4HSAOGKfxZHs8ziSfR5fiMpnYW0UxhhjwrIzCmOMMWFZoTDGGBOWFYo4IiKDRWRxaNrX9SJyi9OZnCYi7tB8JAudzuK0WE0N3FWIyPdC/598JiJ/E5EkpzPFkog8KiKlIvJZk3URTTPdXlYo4ksjcKuqjgOOB74tIuMdzuS0W4jSFLhdUEymBu4KRGQgcDOQr6oTATdwmbOpYu4xYE6zdZFOM90uVijiiKruVtVPQ68rCX4RDHQ2lXNEZBBwDvCw01mc1mRq4EcgODWwqh5yNpXjPECyiHiAFGCXw3liSlWXAgearZ5HcHppQs/nd8axrFDEKREZCkwFPg6/Zbf2O+AHQKCtDXuAdk0N3N2p6ufAr4AiYDdQrqpvO5sqLkQyzXS7WaGIQyKSBjwPfFdVK5zO4wQRORcoVdWVTmeJE+2aGri7C117nwcMAwYAqSJylbOpui8rFHFGRLwEi8STqvqC03kcdBLwVREpBJ4GZonIE85GclRLUwMf62Aep80Gdqhqmar6gBeAEx3OFA/2hqaXJsw00+1mhSKOiIgQvAa9UVV/43QeJ6nqHao6SFWHEmyk/Ieq9ti/GG1q4KMUAceLSEro/5sz6MGN+020Oc30l+HIDHemVScBVwPrRGR1aN2dqvp6mJ8xPcfhqYETgALgOofzOEZVPxaR54BPCfYWXEUPG8pDRP4GzARyRaQEuJvgtNLPiMg3CBbTSzrlWDaEhzHGmHDs0pMxxpiwrFAYY4wJywqFMcaYsKxQGGOMCcsKhTHGmLCsUBgTIRHxi8jqJo9OuzNaRIY2HQXUmHhi91EYE7laVZ3idAhjYs3OKIzpIBEpFJFfisgnocfI0PohIrJIRNaGnvNC6/uKyIsisib0ODz0hFtEHgrNsfC2iCSHtr9ZRDaE9vO0Q7+m6cGsUBgTueRml54ubfJehapOB+4nOOotodd/UdVJwJPAfaH19wHvqepkguM1rQ+tHwX8QVUnAIeAi0LrbwemhvZzQ7R+OWNaY3dmGxMhEalS1bQW1hcCs1S1IDSo4x5V7SUi+4D+quoLrd+tqrkiUgYMUtX6JvsYCrwTmnAGEbkN8Krq/4rIm0AV8BLwkqpWRflXNeYIdkZhTOfQVl63tk1L6pu89vNFG+I5wB+A44CVoYl6jIkZKxTGdI5Lmzx/GHr9AV9Mz3klsCz0ehFwI/xrTvCM1nYqIi5gsKouJjiJUxZw1FmNMdFkf5kYE7nkJqP6QnD+6sNdZBNF5GOCf3xdHlp3M/CoiHyf4Ox0h0d7vQVYEBrh00+waOxu5Zhu4AkRyQQE+K1NgWpizdoojOmgUBtFvqruczqLMdFgl56MMcaEZWcUxhhjwrIzCmOMMWFZoTDGGBOWFQpjjDFhWaEwxhgTlhUKY4wxYf0/vnIGrAbkRkkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pn = Perceptron(0.1,10)\n",
    "pn.fit(X_scaled,y)\n",
    "plt.plot(range(1, len(pn.error) + 1), pn.error, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Unit4Spacey",
   "language": "python",
   "name": "unit4spacey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
